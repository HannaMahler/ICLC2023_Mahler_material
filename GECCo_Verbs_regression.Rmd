---
title: "Regression analysis of number of Verb phrases in GECCo corpus"
author: "Hanna Mahler"
date: "30 11 2022"
output: html_document
---

#0. Preamble

Goal of the analysis: We want to investigate the "verbiness" of texts. Which factors influence the frequency of verb phrases (VPs) that are being used within a text? Is the language the deciding factor? Or the register, or the Mode, or the information density?

Why Bayesian? -> This is a domain with many pre-existing assumptions that can inform the analysis. (Next to the other good reasons for using a Bayesian approach!)

#1. Load libraries

```{r, message = FALSE, warning = FALSE}
library(tidyverse) ## needed for data wrangling
library(readxl) ## needed for reading in data
library(brms) ## needed for model fitting
library(performance) ## needed for check_collinearity() below
library(ggmcmc) ## needed for visualisation of prior & posterior distributions
library(stats) ## needed for setting contrasts below.
options(scipen = 999) # this turns off the scientific notation of very high/low numbers (e-10)
set.seed(42) ## make results reproducible be setting seed to a specific number
```

#2. Load data

Load and inspect the pre-processed, tidy data.

```{r}
texts <- read_excel("Overview_texts_vp.xlsx") %>%
  mutate(Language = as.factor(Language),
         Register = as.factor(Register), 
         Mode = as.factor(Mode),
         STTR_z = scale(STTR), ## the variable STTR (standardized type-token-ratio) needs to be z-scored (= centered and scaled)
         NR_tokens_phw = NR_tokens/100) ## we create a column that counts units of 100 words (we need this so that the model predicts vp per hundred words instead of verb phrases per word)

head(texts)
```

The data frame "texts" contains all English and German texts within the corpus as separate rows. The columns contain different pieces of information on each text. Especially relevant for us is the language of the text (Language), its register (Register), its Mode (Mode), the standardized type-token ratio (STTR), the number of verb phrases for each text as an absolute number (NR_vp) and the total number of tokens in a text (NR_tokens).

#3. Explore data graphically

As a first step we want to explore the data graphically. We are interested in the distribution of values for each variable, and also in the relationship between the variables.

(More elaborate plots can be found in the separate script for visualizations)

```{r}
## Number of texts per language
summary(texts$Language)

## Number of texts per register
summary(texts$Register)

## Number of texts per Mode
summary(texts$Mode)

## Histogram of STTR
hist(texts$STTR, breaks = 20)
hist(texts$STTR_z, breaks = 20)

pairs(~ Register + STTR + Mode, data  = texts)

## Histogram of text length
hist(texts$NR_tokens, breaks = 20)

## Histogram of vp
hist(texts$vp_phw, breaks = 20)
hist(texts$NR_vp, breaks = 30)

## average number of verb phrases per hundred words
# overall
mean(texts$vp_phw)
# English only
mean(subset(texts, Language == "English")$vp_phw)
# German only
mean(subset(texts, Language == "German")$vp_phw)
```


#4. Modelling

##4.1 Sum-coding categorical predictor variables

Before we start, we should make sure to sum-code all categorical & binary predictor variables to ease interpretation and to improve model fit.

Sum coding: has the unweighted mean as basis for comparison. The coefficient given for level1 is how much you need to add to the intercept. "The last category [...] will be omitted. Its estimated average can be computed be subtracting both coefficients of 'A' and 'B' from the intercept." (Levshina 2015: 146)
  - is apparently also called "deviation coding": https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/

```{r sum-coding predictor variables}
contrasts(texts$Language) <- contr.sum(2)
contrasts(texts$Register) <- contr.sum(14)
contrasts(texts$Mode) <- contr.sum(2)
```


##4.2 Setting priors for the data


###4.2.1 Thinking about the model and random effects structure

What we want to predict (dependent variable): 
- the "verbiness" of a text, operationalised as number of verb phrases per text (NR_vp)

What we can use as predictors (independent variables):
- the language of the text (Language, binary variable)
- the register of the text (Register, categorical variable)
- the Mode of the text (Mode, binary variable)
- the density of the text, operationalised as z-scored standardized type-token-ration (STTR_z, numerical variable)
- the length of the text, operationalised as number of tokens in units of one hundred words (NR_tokens_phw, numerical variable)

Our model formula
*NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register)*

-> In this model we are predicting counts and can use text length as an "exposure variable" (Winter & Bürkner 2021: 10). The model therefore predicts "counts over units of exposure" (Winter & Bürkner 2021: 11), so in our case per hundred words.

Quick reminder on notation:
(1|variable) specifies varying intercepts
"1 +" stands for the Intercept, can also be omitted.


###4.2.2 Coming up with priors for the fixed and random effects

To set the priors we need to temporarily forget what we learned about the data above. The priors should ONLY be informed by the existing literature/knowledge, not by the actual data set!

Distribution family for the dependent variable:

NR_vp: only positive values make sense
  -> Fischer (2013) finds a rough average of 16 VPs per hundred words (averaged over English and German)

Priors for the independent variables:

What we know from the existing literature:
- English probably uses more VPs than German. One study (Fischer 2013: 169) suggests that English texts have 11% more VPs than German texts.
- in both English and German we expect more VPs in spoken language than in written language.
- in both English and German we expect that texts that are more dense (high STTR) use fewer VPs (more nominal style)


####4.2.3 Setting priors

Default priors make no sense for poisson regression, because of how the link function affects them.

McElreath (2019: 354): "The log link ensures that λi is always positive, which is required of the expected value of a count outcome. But as mentioned in the previous chapter, it also implies an exponential relationship between predictors and the expected value. Exponential relationships grow very quickly, and few natural phenomena can remain exponential for long. So one thing to always check with a log link is whether it makes sense at all ranges of the predictor variables. The priors on the log scale also scale in surprising ways. So prior predictive simulation is again helpful."

Because of the exposure variable, the model output is on the scale "verb phrases per hundred words". This means that the prior also needs to be on (the log of) this ratio scale instead of the count scale. See also Winter & Bürkner (2021: 10).

```{r}
## Use get_prior() to find out about possible priors and their names
get_prior(NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register), 
          data = texts, family = poisson())

## Create a vector of priors using  prior(distribution, class = ..., coef = ...)
priors_original <- c(
  #### prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept),
  #### prior for the slope for Language
  prior(normal(0.1, 0.2), class = b, coef = Language1),
  #### prior for the slope for Mode
  prior(normal(0.1, 0.2), class = b, coef = Mode1),
  #### prior for the slope for STTR_z
  prior(normal(-0.1, 0.2), class = b, coef = STTR_z),
  #### prior for random effect
  prior(normal(0, 0.1), class = sd),
  ## priors for the two interaction terms
  prior(normal(0, 0.1), class = b, coef = Language1:Mode1), 
  prior(normal(0, 0.1), class = b, coef = Language1:STTR_z)
  )
```

##4.3 Prior predictive check

We want to make sure that the priors we set generate useful simulations. For this we use code from McElreath (2020: 356-357) to predict random values from the distribution we specified. Note that the slope needs to be estimated together with the intercept due to the non-linear nature of the transformation (Winter & Bürkner 2021: 15). We therefore use exp(intercept + slope) for each slope term to check whether the priors are reasonable.

```{r prior predictive distribution for the intercept}
mean = 2.8
sd = 0.5
curve(dlnorm(x, mean, sd), from = 0, to = 100, n = 200, xlab = "mean number of VPs per hundred words", y = "Density")
```

```{r prior predictive distribution for slope Mode}
# sum-coded: spoken 1, written -1
N <- 100 # number of prior trends
a <- rnorm(N, 2.8, 0.5) ## here we enter the values from the intercept, see above
b <- rnorm(N, 0.1, 0.2)
plot(NULL, xlim = c(-2, 2), ylim = c(0, 50), ylab = "Count of VPs per hundred words", xlab = "Mode (1 = spoken, -1 = written)")
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col = "grey")
```

```{r prior predictive distribution for slope language}
# sum-coded: English 1, German -1
N <- 100 # number of prior trends
a <- rnorm(N, 2.8, 0.5) ## here we enter the values from the intercept, see above
b <- rnorm(N, 0.1, 0.2) 
plot(NULL, xlim = c(-2, 2), ylim = c(0, 50), ylab = "Count of VPs per hundred words", xlab = "Mode (1 = English, -1 = German)")
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col= "grey" )
```

```{r prior predictive distribution for slope STTR_z}
N <- 100 # number of prior trends
a <- rnorm(N, 2.8, 0.5) ## here we enter the values from the intercept, see above
b <- rnorm(N, -0.1, 0.2) 
plot(NULL, xlim = c(-2, 2), ylim = c(0, 50), ylab = "Count of VPs per hundred words", xlab = "STTR_z")
for ( i in 1:N ) curve(exp(a[i] + b[i]*x), add=TRUE, col = "grey")
```

```{r prior predictive distribution for interaction term}
N <- 70 # number of prior trends
a <- rnorm(N, 2.8, 0.5) ## here we enter the values from the intercept, see above
b <- rnorm(N, 0, 0.1) 
plot(NULL, xlim = c(-4, 4), ylim = c(0, 50), ylab = "Count of VPs per hundred words", xlab = "Interaction term")
for ( i in 1:N ) curve(exp(a[i] + b[i]*x), add=TRUE, col = "grey")
```

```{r prior predictive distribution for random slopes by register}
N <- 60 # number of prior trends
a <- rnorm(N, 2.8, 0.5) ## here we enter the values from the intercept, see above
b <- rnorm(N, 0, 0.1) 
plot(NULL, xlim = c(-4, 4), ylim = c(0, 50), ylab = "Count of VPs per hundred words", xlab = "Register")
for ( i in 1:N ) curve(exp(a[i] + b[i]*x), add=TRUE, col = "grey")
```


##4.4 Fitting model

We fit a poisson regression model using brm() with the priors from above (priors_original).
This should not take more than 3 minutes. If it takes too long you can also load the existing rds-file.

```{r model with English and German}
vp_model_poisson <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register),
  data = texts,
  family = poisson(),
  prior = priors_original,
  warmup = 2000,
  iter = 6000,
  chains = 3,
  #cores = 3
  #control = list(adapt_delta = 0.95),
  )

#vp_model_poisson <- read_rds("Models/vp_model_poisson.rds")
```

There are no convergence issues or warnings.

##4.5 Checking model fit

###4.5.1 Inspecting model output and plots

Because we added an exposure variable, the model predicts "counts over units of exposure" (Winter & Bürkner 2021: 11). So per one hundred words. [But not in the conditional_effects() function, that uses the original count scale!]

Interpretation of the coefficient table in Bayesian statistics: (Winter & Bürkner 2021: 7)
- column 1 "Estimate": the mean of the posterior distribution for this coefficient
- column 2 "Est.Error": the standard deviation of the posterior distribution for this coefficient
- column 3 "l-95% CI": the lower boundary of the 95% credible interval of the coefficient
- column 4 "u-95% CI": the upper boundary of the 95% credible interval of the coefficient
- Group-level effect "sd(Intercept)": an estimate of the variation in intercepts by the random-effects variable

Population-Level Effects: 
                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept             2.55      0.03     2.49     2.62 1.00     3379     4671
Language1             0.06      0.00     0.05     0.06 1.00    12256     7849
Mode1               0.09      0.03     0.03     0.15 1.00     3325     4276
STTR_z               -0.04      0.01    -0.05    -0.03 1.00    11863     8254
Language1:Mode1     0.01      0.00     0.00     0.02 1.00    10535     8167
Language1:STTR_z     -0.01      0.00    -0.02    -0.00 1.00     9683     7379

Explanation of *credible interval*: "In Bayesian statistics, credible intervals indicate the range within which a parameter value falls with a particular probability. More narrow intervals indicate higher precision in one's estimate." (Winter & Bürkner 2021: 7)

```{r}
plot(vp_model_poisson) # check whether the chains have mixed
summary(vp_model_poisson) # inspect the predictions and R-hat values. The R-Hat value should be close to 1 and should not exceed 1.1 (Nalborczyk et al. 2019: 21)
posterior_summary(vp_model_poisson) # to also see the estimates for each level of the random effect and to see more decimals

conditional_effects(vp_model_poisson)
# This function shows the mean value predicted by the model with 95% credible-intervals, for each individual predictors. If not specified otherwise, the function will use the mean or reference level of the other predictors for calculating the predictions. These predictions are on the original count scale.

##check for multicollinearity
check_collinearity(vp_model_poisson)
```

##4.6 Sensitivity analysis

It is recommended to try other priors to see whether the results change significantly (Kruschke 2021: 1288-1289).

The priors I fitted above are *informative*. In the sensitivity analysis we should therefore fit priors that are *uninformative* and *more informative* (i.e. distributions with a different mean). I am also trying priors that are wider and narrower (i.e. that are only altered in their standard deviation)

```{r see priors in the original model}
## use prior_summary(modelname) to see which priors we set beforehand:
prior_summary(vp_model_poisson)
```

```{r specifying uninformative priors}
## set new, uninformative priors (centred at 0)
priors_uninf <- c(
  ## prior for the intercept
  prior(normal(3.5, 1), class = Intercept), # original: normal(2.8, 0.5)
  ## prior for the slope for Language
  prior(normal(0, 0.2), class = b, coef = Language1), # original: normal(0.1, 0.2)
  ## prior for the slope for Mode
  prior(normal(0, 0.2), class = b, coef = Mode1), # original: normal(0.1, 0.2)
  ## prior for the slope for STTR_z
  prior(normal(0, 0.2), class = b, coef = STTR_z), # original: normal(-0.1, 0.2)
  ## prior for random effect
  prior(normal(0, 0.01), class = sd), # original: normal(0, 0.1)
  ## priors for interaction terms
  prior(normal(0, 0.01), class = b, coef = Language1:Mode1), # original: normal(0, 0.1)
  prior(normal(0, 0.01), class = b, coef = Language1:STTR_z) # original: normal(0, 0.1)
  )


## run the model again (same iterations as above)
vp_model_poisson_uninf <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register),
  data = texts,
  family = poisson(),
  prior = priors_uninf,
  warmup = 2000,
  iter = 6000,
  chains = 3,
  #cores = 3
  #control = list(adapt_delta = 0.95),
  )

#vp_model_poisson_uninf <- read_rds("Models/vp_model_poisson_uninf.rds")
```

```{r specifying wider priors}
## set new, wider priors
priors_wider <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept), # original: normal(2.8, 0.5)
  ## prior for the slope for Language
  prior(normal(0.1, 0.4), class = b, coef = Language1), # original: normal(0.1, 0.2)
  ## prior for the slope for Mode
  prior(normal(0.1, 0.4), class = b, coef = Mode1), # original: normal(0.1, 0.2)
  ## prior for the slope for STTR_z
  prior(normal(-0.1, 0.4), class = b, coef = STTR_z), # original: normal(-0.1, 0.2)
  ## prior for random effect
  prior(normal(0, 0.2), class = sd), # original: normal(0, 0.1)
  ## priors for interaction terms
  prior(normal(0, 0.2), class = b, coef = Language1:Mode1), # original: normal(0, 0.1)
  prior(normal(0, 0.2), class = b, coef = Language1:STTR_z) # original: normal(0, 0.1)
  )

## run the model again (same iterations as above)
vp_model_poisson_wider <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register),
  data = texts,
  family = poisson(),
  prior = priors_wider,
  warmup = 2000,
  iter = 6000,
  chains = 3,
  #cores = 3
  #control = list(adapt_delta = 0.95),
  )

#vp_model_poisson_wider <- read_rds("Models/vp_model_poisson_wider.rds")
```

```{r specifying narrower priors}
## set new, narrower priors
priors_narrower <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept), # original: normal(2.8, 0.5)
  ## prior for the slope for Language
  prior(normal(0.1, 0.1), class = b, coef = Language1), # original: normal(0.1, 0.2)
  ## prior for the slope for Mode
  prior(normal(0.1, 0.1), class = b, coef = Mode1), # original: normal(0.1, 0.2)
  ## prior for the slope for STTR_z
  prior(normal(-0.1, 0.1), class = b, coef = STTR_z), # original: normal(-0.1, 0.2)
  ## prior for random effect
  prior(normal(0, 0.05), class = sd), # original: normal(0, 0.1)
  ## priors for interaction terms
  prior(normal(0, 0.01), class = b, coef = Language1:Mode1), # original: normal(0, 0.1)
  prior(normal(0, 0.01), class = b, coef = Language1:STTR_z) # original: normal(0, 0.1)
  )


## run the model again (same iterations as above)
vp_model_poisson_narrower <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register),
  data = texts,
  family = poisson(),
  prior = priors_narrower,
  warmup = 2000,
  iter = 6000,
  chains = 3,
  #cores = 3
  #control = list(adapt_delta = 0.95),
  )

vp_model_poisson_narrower <- read_rds("Models/vp_model_poisson_narrower.rds")
```

```{r specifying more informative priors}
## set new, narrower priors
priors_moreinf <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept), # original: normal(2.8, 0.5)
  ## prior for the slope for Language
  prior(normal(0.3, 0.2), class = b, coef = Language1), # original: normal(0.1, 0.2)
  ## prior for the slope for Mode
  prior(normal(0.3, 0.2), class = b, coef = Mode1), # original: normal(0.1, 0.2)
  ## prior for the slope for STTR_z
  prior(normal(-0.3, 0.2), class = b, coef = STTR_z), # priginal: normal(-0.1, 0.2)
  ## prior for random effect
  prior(normal(0, 0.1), class = sd), # original: normal(0, 0.1)
  ## priors for interaction terms
  prior(normal(0, 0.1), class = b, coef = Language1:Mode1), # original: normal(0, 0.1)
  prior(normal(0, 0.1), class = b, coef = Language1:STTR_z) # original: normal(0, 0.1)
  )


## run the model again (same iterations as above)
vp_model_poisson_moreinf <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register),
  data = texts,
  family = poisson(),
  prior = priors_moreinf,
  warmup = 2000,
  iter = 6000,
  chains = 3,
  #cores = 3
  #control = list(adapt_delta = 0.95),
  )

vp_model_poisson_moreinf <- read_rds("Models/vp_model_poisson_moreinf.rds")
```

Inspect the models with the altered priors. Use the plotting and summary code from above.

```{r inspecting new models}
## model with wider priors
plot(vp_model_poisson_wider)
summary(vp_model_poisson_wider)
conditional_effects(vp_model_poisson_wider)

## model with uninformative priors
plot(vp_model_poisson_uninf)
summary(vp_model_poisson_uninf)
conditional_effects(vp_model_poisson_uninf)

## model with narrower priors
plot(vp_model_poisson_narrower)
summary(vp_model_poisson_narrower)
conditional_effects(vp_model_poisson_narrower)

## model with more informative priors
plot(vp_model_poisson_moreinf)
summary(vp_model_poisson_moreinf)
conditional_effects(vp_model_poisson_moreinf)
```

*How big is the influence of the priors?*
- Specifying uninformative priors returns the same estimates as the original model
- Specifying wider priors returns the same estimates as the original model
- Specifying narrower priors returns the same estimates as the original model
- Specifying more informative priors returns virtually the same estimates as the original model.


##4.7 Visualize effects

It is helpful to visualize the prior and the posterior distribution in one graph. We can do this either on the log-scale or on the exponentiated scale.
- we specify our priors on the "log of number of VPs per hundred words" scale, since they will be exponentiated through the link function
- the model summary presents numbers on the "log of relative number of VPs per hundred words" scale because we added an exposure variable. 
We can exponentiate both to get to the "number of VPs per hundred words" scale. 
 
```{r visualise intercept on log scale}
summary(vp_model_poisson)$fixed[1,3:4] ## copy-paste values into code below

vp_model_poisson_transformed <- ggs(vp_model_poisson) # the ggs function transforms the brms output into a longformat tibble, that we can use to make different types of plots.

## visualisation on log scale
ggplot(filter(vp_model_poisson_transformed, Parameter == "b_Intercept", Iteration > 1000),
       aes(x = value))+ 
  geom_density(fill  = "yellow", alpha = .5)+
  ## prior distribution specified above: rnorm(mean = 2.8, sd = 0.5)
  geom_density(mapping = aes((rnorm(15000, 2.8, 0.5))), fill = "green", alpha = 0.5) +
  geom_vline(xintercept = 0, col = "red", linewidth = 1) +
  scale_x_continuous(name = "Intercept estimate on log-scale", limits = c(0, 4)) + 
  geom_vline(xintercept = c(2.487, 2.617), ## enter values from above here
             col = "blue", linetype = 2) +
  theme_light() +
  labs(title = "Prior & Posterior Density of Intercept")
```

```{r visualise intercept on vp_phw scale}
## credible intervals:
summary(vp_model_poisson)$fixed[1,3:4] ## copy-paste values into code below

vp_model_poisson_transformed <- ggs(vp_model_poisson) # the ggs function transforms the brms output into a longformat tibble, that we can use to make different types of plots.

## visualization on vp_phw scale
ggplot(filter(vp_model_poisson_transformed, Parameter == "b_Intercept", Iteration > 1000),
       ## add the model estimates
       aes(x = exp(value)), trim = TRUE) + 
  geom_density(fill  = "yellow", alpha = 0.5, trim = TRUE)+
  ## add prior distribution specified above: rnorm(mean = 2.8, sd = 0.5)
  geom_density(mapping = aes((exp(rnorm(15000, 2.8, 0.5)))), 
               fill = "green", alpha = 0.5, trim = TRUE) +
  ## add a line at the intercept
  geom_vline(xintercept = 0, col = "red", linewidth = 1) +
  ## set x-axis limits
  scale_x_continuous(name = "Intercept estimate on scale: Verb phrases per hundred words", limits = c(0, 35)) +
  scale_y_continuous(name = "Density", limits = c(0, 1)) +
  ## add vertical lines for the credible intervals
  geom_vline(xintercept = c(exp(2.487), exp(2.61)), ## enter values from above here (exp(value))
             col = "blue", linetype = 2) +
  theme_light() +
  labs(title = "Prior and Posterior Density of Intercept")

```

```{r visual actual data distribution}
ggplot()+
  ## add the actual distribution of the data (does not work at the moment for unknown reasons)
  geom_density(mapping = aes(x = texts$vp_phw), fill = "orange", alpha = 0.5, trim = TRUE) + 
  ## add a line at the intercept
  geom_vline(xintercept = 0, col = "red", linewidth = 1) +
  ## set x-axis limits
  scale_x_continuous(name = "Intercept estimate on scale: Verb phrases per hundred words", limits = c(0, 35)) +
  scale_y_continuous(name = "Density", limits = c(0, 1)) +
  ## add vertical lines for the credible intervals
  theme_light() +
  labs(title = "Prior and Posterior Density of Intercept")
```


For the slopes we visualise the estimate on the log scale to ease comparison. If we wanted to exponentiate we would have to add the intercept, exp(Intercept + slope), thereby no longer looking at the difference between conditions.

```{r visualise fixed effect STTR_z on log scale}
summary(vp_model_poisson)$fixed[4,3:4]

ggplot(filter(vp_model_poisson_transformed, Parameter == "b_STTR_z", Iteration > 1000), 
       mapping = aes(x = value)) +
  geom_density(fill = "orange", alpha = 0.5)+
  geom_density(mapping = aes((rnorm(15000, -0.1, 0.2))), fill = "green", alpha = 0.5) + ## enter prior specifications from above
  geom_vline(xintercept = 0, col = "red", linewidth = 1)+
  scale_x_continuous(name = "Estimate for STTR_z coefficient on log-scale", limits = c(-0.5, 0.1))+ 
  geom_vline(xintercept = c(-0.0542, -0.0323), col = "blue", linetype = 2)+ ## enter values from above here
  theme_light()+
  labs(title = "Posterior and Prior Density of Regression Coefficient for STTR_z")
```

```{r visualise fixed effect Mode on log scale}
summary(vp_model_poisson)$fixed[3,3:4]

ggplot(filter(vp_model_poisson_transformed, Parameter == "b_Mode1", Iteration > 1000), aes(x = value))+
  geom_density(fill = "orange", alpha = .5)+
  geom_vline(xintercept = 0, col = "red", linewidth = 1)+
  geom_density(mapping = aes(rnorm(15000, 0.1, 0.2)), fill = "green", alpha = 0.5) + ## enter prior specifications from above normal(0.1, 0.2)
  scale_x_continuous(name = "Estimate for Mode coefficient on log-scale", limits = c(-0.5, 0.6))+ 
  geom_vline(xintercept = c(0.0275, 0.1543), col = "blue", linetype = 2)+ ## enter values from above here
  theme_light()+
  labs(title = "Posterior and Prior Density of Regression Coefficient for Mode")
```

```{r visualise fixed effect language on log scale}
summary(vp_model_poisson)$fixed[2,3:4]

ggplot(filter(vp_model_poisson_transformed, Parameter == "b_Language1", Iteration > 1000), aes(x = value))+
  geom_density(fill = "orange", alpha = .5)+
  geom_density(mapping = aes(rnorm(15000, 0.1, 0.2)), fill = "green", alpha = 0.5) + ## enter prior specifications from above
  geom_vline(xintercept = 0, col = "red", linewidth = 1)+
  scale_x_continuous(name = "Estimate for Language coefficient on log-scale", limits = c(-0.005, 0.51))+ 
  geom_vline(xintercept = c(0.0514, 0.0638), col = "blue", linetype = 2)+ ## enter values from above here
  theme_light()+
  labs(title = "Prior and Posterior Density of Regression Coefficient for Language")
```


##4.8 Posterior predictive checks

We use the posterior distribution to create new data.

```{r}
pp_check(vp_model_poisson, ndraws = 100, type = "dens_overlay")
pp_check(vp_model_poisson_wider, ndraws = 100, type = "dens_overlay")
pp_check(vp_model_poisson_narrower, ndraws = 100, type = "dens_overlay")
pp_check(vp_model_poisson_moreinf, ndraws = 100, type = "dens_overlay")
pp_check(vp_model_poisson_uninf, ndraws = 100, type = "dens_overlay")

pp_ecdf_original <- pp_check(vp_model_poisson, ndraws =  100, type = 'ecdf_overlay')
pp_ecdf_wider <- pp_check(vp_model_poisson_wider, ndraws =  100, type = 'ecdf_overlay')
pp_ecdf_narrower <- pp_check(vp_model_poisson_narrower, ndraws =  100, type = 'ecdf_overlay')
pp_ecdf_moreinf <- pp_check(vp_model_poisson_moreinf, ndraws =  100, type = 'ecdf_overlay')
pp_ecdf_uninf <- pp_check(vp_model_poisson_uninf, ndraws =  100, type = 'ecdf_overlay')

pp_ecdf_original
pp_ecdf_wider
pp_ecdf_narrower
pp_ecdf_moreinf
pp_ecdf_uninf
```


```{r}
predict(object = vp_model_poisson, newdata = NULL, re_formula = NULL, summary = TRUE)
## this returns a data frame with 372 rows (same as the original "texts" df) and columns for estimate, standard error and quantiles.

## predict values for specific texts
newdata <- data.frame(Language = factor(c("English", "German")), 
                      Register = factor(c("ESSAY", "ESSAY")), 
                      Mode = factor(c("written", "written")), 
                      NR_tokens_phw = c(1, 1), 
                      STTR_z = c(0,0))
newdata
predict(vp_model_poisson, newdata = newdata)
```



##4.9 Comparing coefficient size / hypothesis testing

I am doing one-sided hypothesis testing. Extract from the Vignette: "For a one-sided hypothesis, [the evidence ratio] is just the posterior probability (Post.Prob) under the hypothesis against its alternative. That is, when the hypothesis is of the form a > b, the evidence ratio is the ratio of the posterior probability of a > b and the posterior probability of a < b. In this example, values greater than one indicate that the evidence in favor of a > b is larger than evidence in favor of a < b."

So: 
- If Evid.Ratio > 1: there is more evidence for A > B than there is for B > A.
- If Evid.Ratio < 1: there is more evidence for B > A than there is for A > B.
- Evid.Radio of Inf indicates a very large evidence in favor of the tested hypothesis

I am doing Poisson regression, which means that the parameter estimates are on a log-scale. If we want the hypothesis-function to return predictions on the original count scale, we need to include the exp() function wrapped around both intercept and slope. For more details see Winter & Bürkner (2021: 8-9)

```{r}
## Is the effect of language stronger than the effect of Mode?
hypothesis(vp_model_poisson, 'Language1 > Mode1') 
# Evid.Ratio of 0.17 indicates that probably Mode is a stronger effect than language

## Is the effect of language stronger than the effect of density?
hypothesis(vp_model_poisson, 'Language1 > STTR_z')
# Evid.Ratio of Inf indicates that Language is stronger effect than Density

## Is the effect of Mode stronger than the effect of density?
hypothesis(vp_model_poisson, 'Mode1 > STTR_z')
# Evid.Ratio of 11999 indicates that Mode is a stronger effect than Density
```


#5. Model comparison

For the steps above we used this model:
NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + (1|Register)
This is the model that includes all predictors that are relevant from a theoretical perspective. There are, however, other possible models for this data set. For the sake of completeness, I am fitting these models below and comparing them to the theoretically informed model. If the fitting takes too long you can also load the existing rds-files.

Comparison of different models: Nalborczyk et al. (2019: 34-37), Nicenboim & Vasishth (2016: 606-607), McElreath (2020: 229-234)

```{r vp model 1}
priors_1 <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept),
  ## prior for the slope for Language
  prior(normal(0.1, 0.2), class = b, coef = Language1)
  )

vp_model_1 <- brm(
  NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language,
  data = texts,
  family = poisson(),
  prior = priors_1,
  warmup = 2000,
  iter = 6000,
  chains = 3
  )

#vp_model_1 <- read_rds("Models/vp_model_1.rds")

summary(vp_model_1)
plot(vp_model_1)
```

```{r vp model 2}
priors_2 <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept),
  ## prior for the slope for Language
  prior(normal(0.1, 0.2), class = b, coef = Language1),
  ## prior for the slope for Mode
  prior(normal(0.1, 0.2), class = b, coef = Mode1)
  )

vp_model_2 <- brm(
  NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language + Mode,
  data = texts,
  family = poisson(),
  prior = priors_2,
  warmup = 2000,
  iter = 6000,
  chains = 3
  )

#vp_model_2 <- read_rds("Models/vp_model_2.rds")

summary(vp_model_2)
plot(vp_model_2)
```

```{r vp model 3}
priors_3 <- priors_original <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept),
  ## prior for the slope for Language
  prior(normal(0.1, 0.2), class = b, coef = Language1),
  ## prior for the slope for Mode
  prior(normal(0.1, 0.2), class = b, coef = Mode1),
  ## prior for the slope for STTR_z
  prior(normal(-0.1, 0.2), class = b, coef = STTR_z)
)

vp_model_3 <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)),
  data = texts,
  family = poisson(),
  prior = priors_3,
  warmup = 2000,
  iter = 6000,
  chains = 3
  )

#vp_model_3 <- read_rds("Models/vp_model_3.rds")

summary(vp_model_3)
plot(vp_model_3)
```

```{r vp model 4}
priors_4 <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept),
  ## prior for the slope for Language
  prior(normal(0.1, 0.2), class = b, coef = Language1),
  ## prior for the slope for Mode
  prior(normal(0.1, 0.2), class = b, coef = Mode1),
  ## prior for the slope for STTR_z
  prior(normal(-0.1, 0.2), class = b, coef = STTR_z),
  ## prior for interaction term
  prior(normal(0, 0.1), class = b, coef = Language1:Mode1)
  )

vp_model_4 <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode,
  data = texts,
  family = poisson(),
  prior = priors_4,
  warmup = 2000,
  iter = 6000,
  chains = 3
  )

vp_model_4 <- read_rds("Models/vp_model_4.rds")

summary(vp_model_4)
plot(vp_model_4)
```

```{r vp model 5}
priors_5 <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept),
  ## prior for the slope for Language
  prior(normal(0.1, 0.2), class = b, coef = Language1),
  ## prior for the slope for Mode
  prior(normal(0.1, 0.2), class = b, coef = Mode1),
  ## prior for the slope for STTR_z
  prior(normal(-0.1, 0.2), class = b, coef = STTR_z),
  ## priors for interaction terms
  prior(normal(0, 0.1), class = b, coef = Language1:Mode1), 
  prior(normal(0, 0.1), class = b, coef = Language1:STTR_z)
  )

vp_model_5 <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z,
  data = texts,
  family = poisson(),
  prior = priors_5,
  warmup = 2000,
  iter = 6000,
  chains = 3,
  )

#vp_model_5 <- read_rds("Models/vp_model_5.rds")

summary(vp_model_5)
plot(vp_model_5)
```

```{r vp model 6}
vp_model_6 <- vp_model_poisson

summary(vp_model_6)
plot(vp_model_6)
```

```{r vp model 7}
priors_7 <- c(
  ## prior for the intercept
  prior(normal(2.8, 0.5), class = Intercept),
  ## prior for the slope for Language
  prior(normal(0.1, 0.2), class = b, coef = Language1),
  ## prior for the slope for Mode
  prior(normal(0.1, 0.2), class = b, coef = Mode1),
  ## prior for the slope for STTR_z
  prior(normal(-0.1, 0.2), class = b, coef = STTR_z),
  ## prior for the slope for Register
  prior(normal(0, 0.1), class = b),
  ## priors for interaction terms
  prior(normal(0, 0.1), class = b, coef = Language1:Mode1), 
  prior(normal(0, 0.1), class = b, coef = Language1:STTR_z)
  )

vp_model_7 <- brm(
  NR_vp ~ 1 + Language + Mode + STTR_z + offset(log(NR_tokens_phw)) + Language:Mode + Language:STTR_z + Register,
  data = texts,
  family = poisson(),
  prior = priors_7,
  warmup = 2000,
  iter = 6000,
  chains = 3
  )

#vp_model_7 <- read_rds("Models/vp_model_7.rds")

summary(vp_model_7)
plot(vp_model_7)
```


##5.4 Model comparison

Winter & Bürkner (2021: 13): In using leave-one-out cross-validation LOO-CV, we are evaluating models based on how well they predict unseen data. 
ELPD stands for ‘expected log-predictive density’; it is a measure of the expected predictive accuracy of the model, that is, it estimates how well the model is expected to predict unseen data. The model in the first row is always taken as the baseline and the model with the highest ELPD value in the comparison set.

We want to find the model with the best predictive abilities, not necessarily with the best fit to our data (overfitting). This is often done with the leave-one-out-method (LOO).

vp_model_1: NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language
vp_model_2: NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language + Mode
vp_model_3: NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language + Mode + STTR_z
vp_model_4: NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language + Mode + STTR_z + Language:Mode
vp_model_5: NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language + Mode + STTR_z + Language:Mode + Language:STTR_z
vp_model_6: NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language + Mode + STTR_z + Language:Mode + Language:STTR_z + (1|Register)
vp_model_7: NR_vp ~ 1 + offset(log(NR_tokens_phw)) + Language + Mode + STTR_z + Language:Mode + Language:STTR_z + Register

```{r}
vp_model_1_loo <- loo(vp_model_1)
vp_model_2_loo <- loo(vp_model_2)
vp_model_3_loo <- loo(vp_model_3)
vp_model_4_loo <- loo(vp_model_4)
vp_model_5_loo <- loo(vp_model_5)
vp_model_6_loo <- loo(vp_model_6)
vp_model_7_loo <- loo(vp_model_7)

loos <- loo_compare(vp_model_1_loo, vp_model_2_loo, vp_model_3_loo, 
                    vp_model_4_loo, vp_model_5_loo, vp_model_6_loo, vp_model_7_loo)
loos
```

What the comparison tells us: The model we fitted above (vp_model_6) is indeed the best model out of the set, since it is displayed as the baseline. The model using Register as a fixed effect instead of a random effect (vp_model_7) is nearly as good. All the other models without Register perform considerable worse. The two interaction effects only have a minor impact on predictive accuracy.


